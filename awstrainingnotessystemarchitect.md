# AWS notes for system architect Stefane #

AMI is region locked, and same id cannot be used in multiple regions

Can set a security group as the source of security group

Only allow load balancer to talk to ec2 security group, create security group for ec2 with port 80 and only allow the security group of load balancer

What is feature on load balancer that allows our application to have multiple hostnames using SSL? SNI

# Auto scaling groups
* Can be cpu, network or custom
* Use launch configurations, you update asg by providing new launch configuration
* Iam roles attached to an ASG will get assigned to EC2 instances
* ASG free, resources are not
* Instances under ASG means that if they get terminated for whatever reason, the ASG will restart them. Extra safety
* ASG can terminate instances marked as unhealthy by a LB (and hence replace them)

ASG default termination policy
If ASG does scale in which instance would it terminate from below graphic? 
Which has most number of instances
If there are multiple instances choose the one with the oldest launch configuration
ASG tries to balance the number of instances across AZ by default

Application Load Balancers DO NOT HANDLE TCP
Application load balancers can route to different target groups based on all except Client IP

EBS volume types
* GP2 - ssd - general
* Io1 - ssd highest performance
* ST1 - low cost HDD volume
* SCI - lowest cost hdd, less frequently accessed workloads

* EBS volumes are characterized in size | throughput | iops
* When in doubt always consult the AWS documentation - it’s good
* GP2 and IOI only for root volumes

## RDS for solutions architects
* Read replicas are used for SELECT (=READ) only kind of statements, not insert, update and delete
* RDs supports transparent data encryption for db
* Oracle or sql server db instance only
* TDE can be used on top of KMS - may affect performance
* IAM authentication
* Works for mysql, postgresql
* Lifespan of an authentication token is 15 minutes (short lived)
* Tokens are generated by aws credentials
* SSL must be used when connecting to the database
* Easy to use EC2 instance role to connect to the RDS database

# Aurora
* Aurora is proprietary from AWS, not open sources
* Postgres and mysql are both supported as auroradb
* Aurora is aws cloud optimized
* Aurora storage automaticaly grows in increments of 10gb up to 64tb
* Aurora can have 15 replicas while mysql has 5, replication process is faster
* Failover in aurora is instantenous. It’s HA native
* Aurora costs more than RDS

* Aurora high availability and read scaling
* 6 copies of your data across 3 az
* 4 copies out of 6 needed for writes
* 3 copies out of 6 need for reads
* Self healing with peer-to-peer replication
* Storage is striped across 100s of volumes
* Master instance only takes writes, failover happens in roughly 30 seconds
* Can have up to 15 aurora read replicas serve reads 

## Features
* Automatic fail-over
* Backup and recovery
* Isolation And security
* Industry compliance
* Push-button scaling

# Amazon aurora exam tips
* Can use iam authentication for aurora mysql and postgres
* Aurora global databases span multiple regions and enable DR
* One primary region
* One dr region - disaster recovery
* DR region can be used for lower latency reads
* <1 second replica lag on average
* If not using global databases you can create cross-region read replicas 
* But faq recommends global databases instead
# Elasticache
* Managed Redis or Memcached
* Caches are in memory databases with really high performance low latency
* Helps reduce load off of databases for read intnsive workloads
* Helps make your appplication stateless
* Write scaling using sharding
* Read scaling using read replicas
* Multi az with fail over
* Aws manages os maintenance, and everything else like that

User session store, elasticache manages user session data and can read it from central location

* In memory key value store
* Super low latency
* Cache survive reboots by default
* Great to host
* User sesions
* Leaderboard
* Distributed state
* Relieve pressure on databases
* Pub / sub capability fo rmessaging
* Mutli az automatical faiolover for disater recovyer if you don’t want to lose your cache data
* Support for read replicas

# Memcached
In memory
Doesn’t survive reboots
Use caces
Quick retrieval of objects from memory
Cache often accessed objects
Overall redis has largely grown in popularity and has better feature sets than memcached.
Recokmend redis
Aws exam won’t ask which is better, just about elasticache in general

# Elasticache exam tips
## Security
* Redis support redis auth (username / password)
* SSL in-flight encryption must be enabled and used
* Memcached support SASL authentication (advanced)
* None of the caches support IAM authentication
* IAM policies on elasticache are only used to AWS API level security

## Patterns for elasticache

* Lazy loading - all read data is cached data can become stale in cache
* Write-through - adds or update dat in the cache when written to a DB
* Session store - store temporary session data in a cache using ttl features
Quote: there are only two hard things in computer science: cache invalidation and naming things

# Route 53
CNAME - map url to any url, MUST BE SUBDOMAIN
## Alias record - points to aws resource only, root and non route domain
* free 
* Native health check
## Simple routing
* One domain to one url
* Multiple values allowed, and if so they’re returned at random
## Weighted routing policy
* Control the percentage or requests that go to a specific endpoint
* Can be more than 100 but 100 is easier to work with
## Latency routing policy
* Redirect to server that has least latency close to us
* Super helpful when latency is a priority
* Sometimes germany will be re-routed to the US
* Tries to find the closest server
## Health checks
* X checks failed - unhealthy default 3
* X checks passed - heatlhy default 3
* Default health check inverval 30s, can set to 10s but costs more
* About 15 health checkers will check the endpoint health
* One request ever 2 seconds on average
* Can have HTTP, TCP, and HTTPS health checks (no SSL verification)
* Can integrate health check with CloudWatch
* Health  checks can be linked to route53 queries
## Failover routing policy
* Requires health checks
* Has a primary route and if that health check fails send over the secondary ip
## Geo Location routing policy
* Different than latency based
* This is based on user location and not latency
## Multi-value routing policy
* Use when routing traffic to multiple resources
* Want to associate a route 53 health check with records
* up to 8 healthy records are returned for each multi value query
* multi value is not substitue for having an ELB

## Third party domains and route 53
* registrar - organization that manages the reservation of internet domain names
* famous ones
  * godaddy
  * amazon
  * google domains

## How to use third party domain in route53
1. Create a **PUBLIC** hosted zone in route53
2. Update NS records on 3rd party website to use route 53 name servers

Domain registrar != DNS
each registrar usually comes with some form of DNS feature

# Whatisthetime.com - stateless

* Start with t2 micro instance, it will have elastic ip address attached to it
* As more traffic comes in maybe scale up to larger instance such as m5 instance
* May have downtime, scale horizontally, have multiple instances with elastic ip addresses
* Since they have multiple elastic ips you may have difficulties managing all the elastic ip addresses
* Can use route 53 to route between instances but if you remove an instance they will keep hitting that instance for TTL which can be up to an hour and won't be able to access the site for an hour
* Add a load balancer and use private instances, they're launched manually and they're going to use health checks for our ec2 instances, the ELB is public ec2 is private, restrict traffic to ec2 instances via security ggroups
* route53 uses alias record to the ELB, and balances the traffic out, and if an ec2 instance goes down it auto redirects to another instance
* setup auto scaling group for the ec2 instances so that they scale out and in as needed
* Still bad if it's only in one AZ
* Setup load balancer in multi-az and ec2 instances in multi-az so that failure in one az does not bring down the application
* save costs by reserving instances in two AZ's, and then do one demand instances for when the load goes up and down

# myclothes.com - stateful
* allows people to buy clothes online
* shopping cart
* hundreds of users at the same time 
* need to scale, maintain horizontal scalability and keep web app as stateless as possible
* users should not lose their shopping cart
* users should have their details in a database
* For shopping cart if you have multiple instances the cart will be lost unless you add stickyness to the load balancer, or set the cart data in something such as elasticache
* Another way would be to have the user send their cart data but makes http requests larger, and may introduce security risks
* Elasticache is probably a better way to go for high traffic sites for carts, or maybe even dynamodb which we'll go through in another lecture
* if most people are doing reads we can create RDS read replicas to do the read queries, and RDS write for when they need to write to the database in order to scale properly
* for disaster recovery setup RDS to be multi-az as well to ensure that any disasters will have a fail-over
* security - http traffic from all for ELB, for ec2 instances restrict traffic to only from the ELB security group, RDS traffic only from EC2 security group, elasticache traffic only from EC2 security group

# mywordpress.com - stateful
* fully scalable wordpress website
* access and correctly display picture uploads
* user data and blog content should be stored in a mysql database
* multi az load balancer, auto scale ec2, and aurora with mysql and read replicas
* storing images
  * send image through load balancer and it's stored in EBS, and it can be read in EBS
  * if you scale each has their own EBS volume which means all the images aren't synced properly by default
  * we can use EFS for this to store the images, and it will sync between the two ec2 instances automatically, uses NFS for this
* aurora database to have easy Multi-AZ and read-replicas
* storing data in EBS - single instance application
* storing data in EFS - distributed application
* EBS is cheaper but there are a lot of advantages by using EFS

# instantiating applications quickly
* when launching full stack it takes time to
  * install applications
  * insert initial or recovery data
  * configure everything
  * launch application
* ec2 instances
  * golden ami - install applications, os dependencies beforehand and use an ami to launch the instances
  * bootstrap using user data is easy way to do this too, but not always as fast, and kind of slower
  * hybrid - golden ami and user data - elastic beanstalk
* rds databases
  * restore from snapshot 
* ebs volumes
  * restore from a snapshot the disk will already be formatted and have data

# beanstalk overview
* managing intrastructure
* deploying code
* configuring databases
* scaling
* most web apps have same architecture
* you just want code to run

* Beanstalk is developer centric view of deploying an application on aws
* it uses all components that we've seen before but it's all in one view that's easy to make sense of
* it's free and you pay only for resources that you us
* instance configuration and os is handled by beanstalk
* deployment strategy is configurable but performed by elasticbeanstalk
* makes sure that the developer mostly cares only about the application
* three architecture models
* single instance - dev
* lb + asg - great for production and preproduction web apps
* asg - great for non web apps in production - workers, etc
## beantstalk cont.
* has three components
  * application
  * application version gets assigned on each deploy
  * environment name - free naming, usually dev, test prod
* you deploy application versions to environments and can promote application versions to the next environment
* rollback feature to previous application version
* full control over lifecycle of environments
* support for many platforms
* most are supported and if you can't find your platform you can write a custom platform for it 
 
# S3 simple storage solution
* One of the main building blocks of AWS
* Infinitely scaling storage
* widely popular and deserves its own section
* many websites use AWS s3 as a backbone
* many aws services uses AWS s3 as an integration as well

* store objects in buckets
* must have globally unique name
* defined at the region level
* naming convention
  * no upper case
  * no underscore
  * 3-63 characters long
  * not an ip
  * must start with lowercase letter or number
* objects have keys, key is full path
  * <my_bucket>/**my_file.txt**
  * <my_bucket>/**my_folder/another_folder/my_file.txt**
* there's no concept of directories within buckets (UI will trick you into thinking there's directories)
* keys are very long names that can have slashes
* max size is 5TB, if uploading more than 5GB must use multi-part upload
* metadata - list of text key / value pairs - system or user metadata)
* tags - unicode key / value pair - up to 10 - useful for security / lifecycle
* version id (if versioning enabled)

[Back](trainingnotes.md)

